{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_data(file_path: str, url: str):\n",
    "    \"\"\"Download data from the provided URL to the specified file path.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(requests.get(url).text)\n",
    "\n",
    "def build_vocab_set_and_file_length(input_file: str, chunk_size: int = int(1e5)) -> (set, int):\n",
    "    \"\"\"Build a set of unique characters from the entire file and calculate file length.\"\"\"\n",
    "    unique_chars = set()\n",
    "    file_length = 0\n",
    "    with open(input_file, 'r') as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            unique_chars.update(chunk)\n",
    "            file_length += len(chunk)\n",
    "\n",
    "    return unique_chars, file_length\n",
    "\n",
    "\n",
    "def get_mappings(chars: set) -> (set, dict, dict):\n",
    "    \"\"\"Return mappings based on the set of characters.\"\"\"\n",
    "    chars = sorted(list(chars))\n",
    "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    itos = {i: ch for i, ch in enumerate(chars)}\n",
    "    return chars, stoi, itos\n",
    "\n",
    "def encode_chunk(chunk: str, stoi: dict) -> list:\n",
    "    \"\"\"Encode a chunk of text using the stoi mapping.\"\"\"\n",
    "    return [stoi[c] for c in chunk]\n",
    "\n",
    "\n",
    "def process_data_in_chunks(input_file: str, output_dir: str, stoi: dict, file_length: int, block_size: int = 1024, chunk_size: int = 10000, split_ratio: float = 0.98):\n",
    "    \"\"\"Process the dataset in chunks to manage memory usage.\"\"\"\n",
    "    \n",
    "    train_file = os.path.join(output_dir, 'train.bin')\n",
    "    val_file = os.path.join(output_dir, 'val.bin')\n",
    "\n",
    "    # Calculate the split point based on the entire file length\n",
    "    split_point = int(file_length * split_ratio)\n",
    "    split_point -= split_point % block_size\n",
    "\n",
    "    # Initialize counters\n",
    "    data_size_processed = 0\n",
    "    train_tokens_count = 0\n",
    "    val_tokens_count = 0\n",
    "\n",
    "    num_chunks = -(-file_length // chunk_size)  # Ceiling division\n",
    "    with tqdm(total=num_chunks, desc=\"Processing Chunks\") as pbar:\n",
    "        with open(input_file, 'r') as f, open(train_file, 'wb') as train_f, open(val_file, 'wb') as val_f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                encoded_chunk = encode_chunk(chunk, stoi)\n",
    "                chunk_length = len(encoded_chunk)\n",
    "                data_size_processed += chunk_length\n",
    "\n",
    "                # Determine if the chunk crosses the split point\n",
    "                if data_size_processed < split_point:\n",
    "                    train_chunk = np.array(encoded_chunk, dtype=np.uint8)\n",
    "                    train_chunk.tofile(train_f)\n",
    "                    train_tokens_count += chunk_length\n",
    "                elif data_size_processed - chunk_length < split_point:\n",
    "                    # Split the chunk into train and val\n",
    "                    train_end = split_point - (data_size_processed - chunk_length)\n",
    "                    train_chunk = np.array(encoded_chunk[:train_end], dtype=np.uint8)\n",
    "                    val_chunk = np.array(encoded_chunk[train_end:], dtype=np.uint8)\n",
    "                    train_chunk.tofile(train_f)\n",
    "                    val_chunk.tofile(val_f)\n",
    "                    train_tokens_count += train_end\n",
    "                    val_tokens_count += chunk_length - train_end\n",
    "                else:\n",
    "                    val_chunk = np.array(encoded_chunk, dtype=np.uint8)\n",
    "                    val_chunk.tofile(val_f)\n",
    "                    val_tokens_count += chunk_length\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"train has {train_tokens_count:,} tokens\")\n",
    "    print(f\"val has {val_tokens_count:,} tokens\")\n",
    "    print(f\"Processed {data_size_processed:,} characters.\")\n",
    "\n",
    "    print(f\"train / blocksize = {train_tokens_count / block_size}\")\n",
    "    print(f\"val / blocksize = {val_tokens_count / block_size}\")\n",
    "    print(f\"total / block_size = {data_size_processed / block_size}\")\n",
    "\n",
    "# Usage\n",
    "# input_file_path = os.path.join(os.path.dirname(__file__), 'input.txt')\n",
    "directory_path = os.getcwd()\n",
    "output_dir = directory_path\n",
    "input_file_path = os.path.join(directory_path, 'input.txt')\n",
    "data_url = 'https://adam-karvonen-chess.s3.us-east-2.amazonaws.com/180k_even_chess_moves.txt'\n",
    "# download_data(input_file_path, data_url)\n",
    "# process_data_in_chunks(input_file_path, os.path.dirname(__file__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time 4.124641418457031e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "download_data(input_file_path, data_url)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time\", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time 1.44722580909729\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "unique_chars, file_length = build_vocab_set_and_file_length('input.txt', int(1e5))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time\", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 184,263,680\n",
      "Total time 0.12897920608520508\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total time\", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 184,263,680\n",
      "all the unique characters: \n",
      " #+-.0123456789=BKNOQRabcdefghx\n",
      "vocab size: 32\n",
      "Total time 1.5217978954315186\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time\", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "184263680\n",
      "179945.0\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_chars))\n",
    "print(file_length)\n",
    "print(file_length/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " #+-.0123456789=BKNOQRabcdefghx\n",
      "vocab size: 32\n"
     ]
    }
   ],
   "source": [
    "chars, stoi, itos = get_mappings(unique_chars)\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# Saving metadata\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'meta.pkl'), 'wb') as f:\n",
    "        pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 185/185 [00:10<00:00, 17.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 180,578,304 tokens\n",
      "val has 3,685,376 tokens\n",
      "Processed 184,263,680 characters.\n",
      "train / blocksize = 176346.0\n",
      "val / blocksize = 3599.0\n",
      "total / block_size = 179945.0\n",
      "Total time 10.360989332199097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "process_data_in_chunks(input_file_path, output_dir, stoi, file_length, chunk_size=int(1e6))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time\", (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3599.0\n"
     ]
    }
   ],
   "source": [
    "val = 3685376\n",
    "print(val / 1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
