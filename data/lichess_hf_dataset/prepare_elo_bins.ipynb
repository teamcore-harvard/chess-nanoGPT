{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "# number of workers in .map() call\n",
    "# good number to use is ~order number of cpu cores // 2\n",
    "num_proc = 14\n",
    "dtype = np.uint8  # Currently there are only 32 tokens in the chess LLMs vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"adamkarvonen/chess_games\"\n",
    "file_path = \"lichess_200k_elo_bins.zip\"\n",
    "# file_path = \"smaller_pgn_file_blocks.zip\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_path, data_files=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0.1': None,\n",
       " 'Unnamed: 0': 163357,\n",
       " 'WhiteElo': 694,\n",
       " 'BlackElo': 830,\n",
       " 'Result': '1-0',\n",
       " 'transcript': '1.d4 Nc6 2.c3 Nf6 3.f3 b6 4.e4 Ba6 5.Bxa6 e6 6.Bb5 Nxd4 7.cxd4 Bb4+ 8.Bd2 a5 9.Bxb4 axb4 10.Ne2 Qe7 11.e5 Nd5 12.a3 c6 13.Bc4 b3 14.Bxb3 Ne3 15.Qd3 Nxg2+ 16.Kf2 Qg5 17.Rg1 Ne3 18.Rxg5 b5 19.Kxe3 f6 20.a4 fxg5 21.axb5 c5 22.dxc5 O-O 23.Qe4 Rfc8 24.Rxa8 Rxa8 25.Qxa8+ Kf7 26.b6 Kg6 27.b7 Kh5 28.b8=Q Kh4 29.Qd6 Kh3 30.Qh8 Kxh2 31.Qxh7+ Kg2 32.Qxe6 Kf1 33.Qeh3+ Ke1 34.Qc2 g4 35.Qxg4 g6 36.Qg3+ Kf1 37.Qf2#',\n",
       " 'elo_bin': '[600, 700)'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[1000, 1100)',\n",
       " '[1100, 1200)',\n",
       " '[1200, 1300)',\n",
       " '[1300, 1400)',\n",
       " '[1400, 1500)',\n",
       " '[1500, 1600)',\n",
       " '[1600, 1700)',\n",
       " '[1700, 1800)',\n",
       " '[1800, 1900)',\n",
       " '[1900, 2000)',\n",
       " '[2000, 2100)',\n",
       " '[2100, 2200)',\n",
       " '[2200, 2300)',\n",
       " '[2300, 2400)',\n",
       " '[2400, 2500)',\n",
       " '[2500, 2600)',\n",
       " '[2600, 2700)',\n",
       " '[2700, 2800)',\n",
       " '[2800, 2900)',\n",
       " '[2900, 3000)',\n",
       " '[3000, 3100)',\n",
       " '[3100, 3200)',\n",
       " '[600, 700)',\n",
       " '[700, 800)',\n",
       " '[800, 900)',\n",
       " '[900, 1000)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo_bins = dataset['train']['elo_bin']\n",
    "bins = set(elo_bins)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elo_bin_idx = defaultdict(list)\n",
    "for idx, elo in enumerate(elo_bins):\n",
    "    elo_bin_idx[elo].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ranges = [[600, 1100], [1100, 1500], [1500, 1900]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600, 1100] has number of games:  632275\n",
      "[1100, 1500] has number of games:  1600000\n",
      "[1500, 1900] has number of games:  1600000\n"
     ]
    }
   ],
   "source": [
    "training_range_idx = defaultdict(list)\n",
    "for tr in training_ranges:\n",
    "    for b in bins:\n",
    "        bin_range = eval(b.replace('[', '('))\n",
    "        if bin_range[0] >= tr[0] and bin_range[0] < tr[1]:\n",
    "            training_range_idx[str(tr)] += (elo_bin_idx[b])\n",
    "\n",
    "for t in training_range_idx:\n",
    "    print(t, 'has number of games: ', len(training_range_idx[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [dataset['train'].select(training_range_idx[t]) for t in training_range_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 210176343 tokens\n",
      "(210176343,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing /home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/train_[600, 1100].bin: 100%|██████████| 1024/1024 [00:04<00:00, 210.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val has 2124032 tokens\n",
      "(2124032,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing /home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/val_[600, 1100].bin: 100%|██████████| 1024/1024 [00:01<00:00, 660.63it/s]\n",
      "tokenizing the splits (num_proc=14): 100%|██████████| 1584000/1584000 [00:20<00:00, 77333.35 examples/s] \n",
      "tokenizing the splits (num_proc=14): 100%|██████████| 16000/16000 [00:00<00:00, 21466.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 578241897 tokens\n",
      "(578241897,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing /home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/train_[1100, 1500].bin: 100%|██████████| 1024/1024 [00:10<00:00, 100.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val has 5843321 tokens\n",
      "(5843321,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing /home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/val_[1100, 1500].bin: 100%|██████████| 1024/1024 [00:01<00:00, 635.63it/s]\n",
      "tokenizing the splits (num_proc=14): 100%|██████████| 1584000/1584000 [00:20<00:00, 78212.63 examples/s] \n",
      "tokenizing the splits (num_proc=14): 100%|██████████| 16000/16000 [00:00<00:00, 21273.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 628669971 tokens\n",
      "(628669971,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing /home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/train_[1500, 1900].bin: 100%|██████████| 1024/1024 [00:10<00:00, 98.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val has 6342299 tokens\n",
      "(6342299,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing /home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/val_[1500, 1900].bin: 100%|██████████| 1024/1024 [00:01<00:00, 621.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, t in enumerate(training_range_idx):\n",
    "    dataset = datasets[idx]\n",
    "    # by default only contains the 'train' split, so create a test split\n",
    "    split_dataset = dataset.train_test_split(\n",
    "        test_size=0.01, seed=2357, shuffle=True\n",
    "    )\n",
    "    split_dataset[\"val\"] = split_dataset.pop(\"test\")  # rename the test split to val\n",
    "\n",
    "    # this results in:\n",
    "    # >>> split_dataset\n",
    "    # DatasetDict({\n",
    "    #     train: Dataset({\n",
    "    #         features: ['text'],\n",
    "    #         num_rows: 8009762\n",
    "    #     })\n",
    "    #     val: Dataset({\n",
    "    #         features: ['text'],\n",
    "    #         num_rows: 4007\n",
    "    #     })\n",
    "    # })\n",
    "\n",
    "    # we now want to tokenize the dataset. Using meta.pkl in the same directory as this file\n",
    "    meta_path = os.path.join(os.path.dirname(__file__), \"meta.pkl\")\n",
    "    # meta_path = '/home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/meta.pkl'\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "\n",
    "    stoi = meta[\"stoi\"]\n",
    "    itos = meta[\"itos\"]\n",
    "\n",
    "    # to read the bin files later, e.g. with numpy:\n",
    "    # m = np.memmap('train.bin', dtype=np.uint8, mode='r')\n",
    "    # print(split_dataset[\"val\"][0])\n",
    "    # print(len(split_dataset[\"val\"][\"transcript\"][0]))\n",
    "\n",
    "    # For verifying that all games are 1024 tokens long\n",
    "    # for game in split_dataset[\"train\"][\"transcript\"]:\n",
    "    #     if len(game) != 1024:\n",
    "    #         print(len(game))\n",
    "    #         print(game)\n",
    "    #         break\n",
    "    # print(stoi)\n",
    "\n",
    "    column_name = \"transcript\"\n",
    "\n",
    "    def process(example):\n",
    "        ids = np.array([stoi[c] for c in example[column_name]], dtype=dtype)\n",
    "        out = {\"ids\": ids, \"len\": len(ids)}\n",
    "        return out\n",
    "\n",
    "    # tokenize the dataset\n",
    "    tokenized = split_dataset.map(\n",
    "        process,\n",
    "        remove_columns=[column_name],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=num_proc,\n",
    "    )\n",
    "\n",
    "    # print(tokenized[\"val\"][\"ids\"])\n",
    "\n",
    "    # concatenate all the ids in each dataset into one large file we can use for training\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset[\"len\"], dtype=np.uint64)\n",
    "        print(f\"{split} has {arr_len} tokens\")\n",
    "        # dname = '/home/ezipe/git/chess_transformer_mothership/chess-nanoGPT/data/lichess_hf_dataset/'\n",
    "        dname = os.path.dirname(__file__)\n",
    "        filename = os.path.join(dname, f\"{split}_{t[0]}_{t[1]}.bin\")\n",
    "        \n",
    "        arr = np.memmap(filename, dtype=dtype, mode=\"w+\", shape=(arr_len,))\n",
    "        print(arr.shape)\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f\"writing {filename}\"):\n",
    "            # Batch together samples for faster write\n",
    "            batch = dset.shard(\n",
    "                num_shards=total_batches, index=batch_idx, contiguous=True\n",
    "            ).with_format(\"numpy\")\n",
    "            # print(batch[0])\n",
    "            arr_batch = np.concatenate(batch[\"ids\"])\n",
    "            # print(arr_batch)\n",
    "            # print(arr_batch.shape)\n",
    "            # Write into mmap\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
