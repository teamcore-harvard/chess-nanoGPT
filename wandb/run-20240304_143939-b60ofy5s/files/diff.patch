diff --git a/train.py b/train.py
index c8c9d81..cd17e6b 100644
--- a/train.py
+++ b/train.py
@@ -77,6 +77,10 @@ config_keys = [k for k,v in globals().items() if not k.startswith('_') and isins
 exec(open('configurator.py').read()) # overrides from command line or config file
 config = {k: globals()[k] for k in config_keys} # will be useful for logging
 # -----------------------------------------------------------------------------
+low_elo = 600
+high_elo = 1100
+
+
 
 # various inits, derived attributes, I/O setup
 ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?
@@ -113,8 +117,8 @@ ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=
 
 # poor man's data loader
 data_dir = os.path.join('data', dataset)
-train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint8, mode='r')
-val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint8, mode='r')
+train_data = np.memmap(os.path.join(data_dir, f'train_{low_elo}_{high_elo}.bin'), dtype=np.uint8, mode='r')
+val_data = np.memmap(os.path.join(data_dir, f'val_{low_elo}_{high_elo}.bin'), dtype=np.uint8, mode='r')
 def get_batch(split):
     data = train_data if split == 'train' else val_data
     # ix = torch.randint(len(data) - block_size, (batch_size,))
